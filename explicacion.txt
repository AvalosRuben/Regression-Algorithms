===============================================================================
                    EXPLICACIÃ“N DETALLADA DEL ALGORITMO MINI-BATCH GRADIENT DESCENT
                              AnÃ¡lisis LÃ­nea por LÃ­nea - Paso a Paso
===============================================================================

AUTOR: AnÃ¡lisis para Proyecto #1 - Algoritmos de RegresiÃ³n
FECHA: 28 de Septiembre, 2025
CURSO: Inteligencia Artificial - Universidad Modelo

===============================================================================
                                    ÃNDICE
===============================================================================

1. IMPORTACIÃ“N DE LIBRERÃAS Y CONFIGURACIÃ“N INICIAL
2. GENERACIÃ“N DE DATOS ARTIFICIALES (NUBE DE PUNTOS)
3. FUNCIONES DE COSTO Y GRADIENTE
4. IMPLEMENTACIÃ“N DEL ALGORITMO MINI-BATCH GRADIENT DESCENT
5. ALGORITMOS DE COMPARACIÃ“N (BATCH GD Y STOCHASTIC GD)
6. ENTRENAMIENTO Y ANÃLISIS COMPARATIVO
7. ANÃLISIS DE HIPERPARÃMETROS
8. PROBLEMAS COMUNES Y SOLUCIONES
9. RECOMENDACIONES Y MEJORES PRÃCTICAS

===============================================================================
1. IMPORTACIÃ“N DE LIBRERÃAS Y CONFIGURACIÃ“N INICIAL
===============================================================================

LÃNEAS 40-46:
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import time
import warnings
warnings.filterwarnings('ignore')
```

EXPLICACIÃ“N LÃNEA POR LÃNEA:

â€¢ import numpy as np
  â””â”€ QUE HACE: Importa NumPy para operaciones matemÃ¡ticas vectorizadas
  â””â”€ POR QUÃ‰ ESTÃ: NumPy es FUNDAMENTAL para operaciones con matrices y vectores
     - MultiplicaciÃ³n matricial: X.dot(theta)
     - Operaciones elemento por elemento: (predictions - y)**2
     - Funciones matemÃ¡ticas: np.random.randn(), np.sum(), etc.
  â””â”€ QUE PASA SI LO CAMBIAS/QUITAS:
     âŒ SIN NUMPY: El cÃ³digo NO funcionarÃ¡ - todas las operaciones matriciales fallarÃ¡n
     âŒ USAR LISTAS DE PYTHON: 100x mÃ¡s lento, cÃ³digo mÃ¡s complejo
  â””â”€ PROBLEMAS POTENCIALES:
     - VersiÃ³n incompatible de NumPy puede causar errores
     - Sin NumPy: TypeError en operaciones matriciales

â€¢ import matplotlib.pyplot as plt  
  â””â”€ QUE HACE: Importa Matplotlib para crear grÃ¡ficos
  â””â”€ POR QUÃ‰ ESTÃ: Necesario para visualizar:
     - Nube de puntos original
     - FunciÃ³n de costo vs iteraciones
     - ComparaciÃ³n entre algoritmos
     - Ajuste de la lÃ­nea de regresiÃ³n
  â””â”€ QUE PASA SI LO CAMBIAS/QUITAS:
     âŒ SIN MATPLOTLIB: No se mostrarÃ¡n grÃ¡ficos (plt.show() fallarÃ¡)
     âœ… ALTERNATIVAS: seaborn, plotly (requieren cambios de sintaxis)
  â””â”€ PROBLEMAS POTENCIALES:
     - Backend no configurado: puede no mostrar grÃ¡ficos
     - Memoria insuficiente para grÃ¡ficos grandes

â€¢ from sklearn.preprocessing import StandardScaler
  â””â”€ QUE HACE: Importa herramientas para normalizaciÃ³n de datos
  â””â”€ POR QUÃ‰ ESTÃ: Para comparar implementaciÃ³n propia vs librerÃ­a
  â””â”€ QUE PASA SI LO CAMBIAS/QUITAS:
     âŒ SIN SKLEARN: No podrÃ¡s usar StandardScaler (pero el algoritmo principal sÃ­ funciona)
     âœ… ALTERNATIVA: Implementar normalizaciÃ³n manualmente
  â””â”€ PROBLEMAS POTENCIALES:
     - Sklearn no instalado: ImportError
     - VersiÃ³n incompatible puede cambiar API

â€¢ import time
  â””â”€ QUE HACE: Importa funciones para medir tiempo de ejecuciÃ³n
  â””â”€ POR QUÃ‰ ESTÃ: Para comparar velocidad entre algoritmos
  â””â”€ QUE PASA SI LO QUITAS:
     âŒ ERROR: time.time() fallarÃ¡ en las funciones de entrenamiento
  â””â”€ PROBLEMAS POTENCIALES: Ninguno, es librerÃ­a estÃ¡ndar

â€¢ warnings.filterwarnings('ignore')
  â””â”€ QUE HACE: Suprime advertencias de librerÃ­as
  â””â”€ POR QUÃ‰ ESTÃ: Para output mÃ¡s limpio (evita warnings de sklearn/numpy)
  â””â”€ QUE PASA SI LO CAMBIAS:
     âš ï¸ QUITAR: MostrarÃ¡n warnings (no afecta funcionalidad)
     âœ… 'default': Muestra advertencias importantes
  â””â”€ PROBLEMAS POTENCIALES:
     - Puede ocultar advertencias importantes
     - En producciÃ³n, mejor manejar warnings especÃ­ficos

===============================================================================
2. GENERACIÃ“N DE DATOS ARTIFICIALES (NUBE DE PUNTOS)
===============================================================================

LÃNEAS 49-70:
```python
np.random.seed(42)
m = 1000  # nÃºmero de muestras
X = 2 * np.random.rand(m, 1)  # feature
y = 4 + 3 * X + np.random.randn(m, 1)  # target con ruido
X_b = np.c_[np.ones((m, 1)), X]  # forma (m, 2)
```

EXPLICACIÃ“N LÃNEA POR LÃNEA:

â€¢ np.random.seed(42)
  â””â”€ QUE HACE: Establece semilla para generar nÃºmeros aleatorios reproducibles
  â””â”€ POR QUÃ‰ EL NÃšMERO 42: Es arbitrario (referencia a "GuÃ­a del Autoestopista GalÃ¡ctico")
  â””â”€ POR QUÃ‰ ESTÃ: REPRODUCIBILIDAD - mismos resultados en cada ejecuciÃ³n
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… OTRO NÃšMERO (0, 123, etc.): Diferentes datos, pero reproducibles
     âŒ SIN SEED: Resultados diferentes cada vez - dificulta debugging
     âš ï¸ SEED DESPUÃ‰S DE IMPORTS: Debe estar antes de generar datos
  â””â”€ PROBLEMAS POTENCIALES:
     - Seed muy tarde: algunos datos ya generados aleatoriamente
     - En producciÃ³n: usar diferentes seeds para validaciÃ³n cruzada

â€¢ m = 1000
  â””â”€ QUE HACE: Define nÃºmero de muestras en el dataset
  â””â”€ POR QUÃ‰ 1000: Balance entre:
     - Suficientes datos para entrenamiento estable
     - No demasiados para que sea rÃ¡pido
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… m = 100: MÃ¡s rÃ¡pido, pero menos estable
     âœ… m = 10000: MÃ¡s estable, pero mÃ¡s lento
     âŒ m = 10: Muy pocas muestras, resultados inestables
     âŒ m = 0: Error - no se pueden crear matrices vacÃ­as
  â””â”€ PROBLEMAS POTENCIALES:
     - m muy pequeÃ±o: underfitting, alta varianza
     - m muy grande: lento, posible memory overflow
     - m no entero: TypeError

â€¢ X = 2 * np.random.rand(m, 1)
  â””â”€ QUE HACE: Genera features X uniformemente distribuidos en [0, 2]
  â””â”€ DESGLOSE:
     - np.random.rand(m, 1): matriz mÃ—1 con valores en [0, 1]
     - 2 *: escala a [0, 2]
     - .shape = (1000, 1): m filas, 1 columna (1 feature)
  â””â”€ POR QUÃ‰ [0, 2]: Rango razonable para visualizaciÃ³n
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… X = np.random.randn(m, 1): DistribuciÃ³n normal (centrada en 0)
     âœ… X = 5 * np.random.rand(m, 1): Rango [0, 5]
     âŒ X = np.random.rand(m): Forma incorrecta (vector 1D)
  â””â”€ PROBLEMAS POTENCIALES:
     - Rango muy grande: problemas de escala en gradientes
     - Forma incorrecta: errores en multiplicaciÃ³n matricial

â€¢ y = 4 + 3 * X + np.random.randn(m, 1)
  â””â”€ QUE HACE: Genera variable objetivo con relaciÃ³n lineal y ruido
  â””â”€ DESGLOSE MATEMÃTICO:
     - 4: intercepto (Î¸â‚€ real)
     - 3: pendiente (Î¸â‚ real)  
     - 3 * X: componente determinÃ­stica
     - np.random.randn(m, 1): ruido gaussiano (Î¼=0, Ïƒ=1)
     - ECUACIÃ“N REAL: y = 4 + 3x + Îµ, donde Îµ ~ N(0,1)
  â””â”€ POR QUÃ‰ ESTOS VALORES:
     - 4, 3: FÃ¡ciles de recordar para verificar si el algoritmo encuentra los valores correctos
     - Ruido gaussiano: Simula datos del mundo real
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… y = 2 + 5*X + 0.5*np.random.randn(m, 1): Menos ruido
     âœ… y = 1 + 2*X: Sin ruido (perfectamente lineal)
     âŒ y = 4 + 3*X + 10*np.random.randn(m, 1): MUCHO ruido - difÃ­cil convergencia
  â””â”€ PROBLEMAS POTENCIALES:
     - Ruido excesivo: el algoritmo no converge bien
     - Sin ruido: sobreajuste, no realista
     - RelaciÃ³n no lineal: algoritmo lineal no ajustarÃ¡ bien

â€¢ X_b = np.c_[np.ones((m, 1)), X]
  â””â”€ QUE HACE: AÃ±ade columna de 1s para el tÃ©rmino de sesgo (bias/intercepto)
  â””â”€ DESGLOSE:
     - np.ones((m, 1)): vector columna de 1s (para Î¸â‚€)
     - np.c_[...]: concatena horizontalmente
     - Resultado: X_b.shape = (1000, 2)
     - X_b = [1  xâ‚]  â† primera fila
              [1  xâ‚‚]  â† segunda fila  
              [â‹®   â‹®]
              [1  xâ‚˜]  â† Ãºltima fila
  â””â”€ POR QUÃ‰ ES NECESARIO:
     - EcuaciÃ³n: y = Î¸â‚€ + Î¸â‚*x = [1, x] Â· [Î¸â‚€, Î¸â‚]áµ€
     - Sin columna de 1s: no hay tÃ©rmino independiente
  â””â”€ QUE PASA SI LO CAMBIAS:
     âŒ SIN COLUMNA DE 1s: La lÃ­nea pasa por el origen (0,0)
     âŒ np.c_[X, np.ones((m, 1))]: Orden incorrecto - Î¸â‚€ y Î¸â‚ intercambiados
     âœ… np.column_stack([np.ones((m, 1)), X]): Equivalente
  â””â”€ PROBLEMAS POTENCIALES:
     - Olvidar columna de bias: modelo sesgado
     - Orden incorrecto: interpretaciÃ³n errÃ³nea de parÃ¡metros

===============================================================================
3. FUNCIONES DE COSTO Y GRADIENTE
===============================================================================

LÃNEAS 72-89:

def compute_cost(X, y, theta):
    m = len(y)
    predictions = X.dot(theta)
    cost = (1/(2*m)) * np.sum((predictions - y)**2)
    return cost

EXPLICACIÃ“N LÃNEA POR LÃNEA:

â€¢ def compute_cost(X, y, theta):
  â””â”€ QUE HACE: Define funciÃ³n para calcular Mean Squared Error (MSE)
  â””â”€ PARÃMETROS:
     - X: matriz de features (m Ã— n)
     - y: vector objetivo (m Ã— 1)
     - theta: parÃ¡metros del modelo (n Ã— 1)
  â””â”€ POR QUÃ‰ ESTA FUNCIÃ“N:
     - Centraliza cÃ¡lculo de costo
     - Reutilizable en diferentes algoritmos
     - Facilita debugging

â€¢ m = len(y)
  â””â”€ QUE HACE: Obtiene nÃºmero de muestras
  â””â”€ POR QUÃ‰ len(y) Y NO X.shape[0]:
     - len(y) mÃ¡s intuitivo
     - y siempre es vector
     - X puede tener diferentes formas segÃºn contexto
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… m = X.shape[0]: Funcionalmente equivalente
     âŒ m = X.shape[1]: ERROR - serÃ­a nÃºmero de features
  â””â”€ PROBLEMAS POTENCIALES:
     - Si X e y tienen diferentes nÃºmeros de filas: error en dot product

â€¢ predictions = X.dot(theta)
  â””â”€ QUE HACE: Calcula predicciones usando multiplicaciÃ³n matricial
  â””â”€ MATEMÃTICA:
     predictions = XÎ¸ = [xâ‚â‚ xâ‚â‚‚] [Î¸â‚] = [xâ‚â‚Î¸â‚ + xâ‚â‚‚Î¸â‚‚]
                       [xâ‚‚â‚ xâ‚‚â‚‚] [Î¸â‚‚]   [xâ‚‚â‚Î¸â‚ + xâ‚‚â‚‚Î¸â‚‚]
                       [ â‹®   â‹® ]        [      â‹®       ]
  â””â”€ POR QUÃ‰ .dot() Y NO *:
     - .dot(): multiplicaciÃ³n matricial
     - *: multiplicaciÃ³n elemento por elemento
  â””â”€ QUE PASA SI LO CAMBIAS:
     âŒ predictions = X * theta: ERROR de forma si no son compatibles
     âœ… predictions = X @ theta: Equivalente (Python 3.5+)
  â””â”€ PROBLEMAS POTENCIALES:
     - Dimensiones incompatibles: ValueError
     - theta no es vector columna: resultado incorrecto

â€¢ cost = (1/(2*m)) * np.sum((predictions - y)**2)
  â””â”€ QUE HACE: Calcula MSE usando la fÃ³rmula matemÃ¡tica
  â””â”€ DESGLOSE MATEMÃTICO:
     - (predictions - y): errores de predicciÃ³n
     - **2: elevamos al cuadrado cada error
     - np.sum(...): suma de errores cuadrÃ¡ticos
     - 1/(2*m): normalizaciÃ³n por nÃºmero de muestras
     - Factor 1/2: simplifica el cÃ¡lculo del gradiente
  â””â”€ FÃ“RMULA MATEMÃTICA:
     J(Î¸) = 1/(2m) Ã— Î£áµ¢â‚Œâ‚áµ (hÎ¸(xáµ¢) - yáµ¢)Â²
  â””â”€ POR QUÃ‰ 1/(2m) Y NO 1/m:
     - 1/2: cuando derivamos, el 2 del exponente cancela el 1/2
     - Simplifica cÃ¡lculo de gradiente: âˆ‚J/âˆ‚Î¸ = 1/m Ã— Xáµ€(XÎ¸ - y)
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… cost = (1/m) * np.sum((predictions - y)**2): MSE estÃ¡ndar (sin 1/2)
     âŒ cost = np.sum((predictions - y)**2): Sin normalizaciÃ³n - sesgado por tamaÃ±o
     âš ï¸ cost = np.mean((predictions - y)**2): Equivalente a MSE estÃ¡ndar
  â””â”€ PROBLEMAS POTENCIALES:
     - DivisiÃ³n por cero si m = 0
     - Overflow si errores muy grandes
     - Underflow si errores muy pequeÃ±os

def compute_gradient(X, y, theta, batch_size=None):
    m = len(y) if batch_size is None else batch_size
    predictions = X.dot(theta)  
    gradient = (1/m) * X.T.dot(predictions - y)
    return gradient

â€¢ def compute_gradient(X, y, theta, batch_size=None):
  â””â”€ QUE HACE: Calcula gradiente de la funciÃ³n de costo
  â””â”€ PARÃMETROS:
     - batch_size=None: usa todos los datos si no se especifica
  â””â”€ POR QUÃ‰ BATCH_SIZE OPCIONAL:
     - Flexibilidad para diferentes tamaÃ±os de batch
     - Mismo cÃ³digo para Batch, Mini-batch y SGD

â€¢ m = len(y) if batch_size is None else batch_size
  â””â”€ QUE HACE: Determina tamaÃ±o efectivo para normalizaciÃ³n
  â””â”€ LÃ“GICA:
     - Si batch_size=None: usa todos los datos (len(y))
     - Si batch_size especificado: usa ese valor
  â””â”€ POR QUÃ‰ ES IMPORTANTE:
     - Batch GD: m = total de muestras  
     - Mini-batch GD: m = tamaÃ±o del batch
     - SGD: m = 1
  â””â”€ QUE PASA SI LO CAMBIAS:
     âŒ m = len(y): Siempre usa total - gradiente incorrecto para mini-batches
     âŒ m = batch_size: Error si batch_size=None
  â””â”€ PROBLEMAS POTENCIALES:
     - batch_size > len(y): sesgo en normalizaciÃ³n
     - batch_size = 0: divisiÃ³n por cero

â€¢ gradient = (1/m) * X.T.dot(predictions - y)
  â””â”€ QUE HACE: Calcula gradiente usando fÃ³rmula matemÃ¡tica
  â””â”€ DESGLOSE:
     - (predictions - y): vector de errores
     - X.T: transpuesta de X (n Ã— m)
     - X.T.dot(...): suma ponderada de errores
     - (1/m): normalizaciÃ³n
  â””â”€ MATEMÃTICA:
     âˆ‡J(Î¸) = 1/m Ã— Xáµ€(XÎ¸ - y)
     
     Para cada parÃ¡metro:
     âˆ‚J/âˆ‚Î¸â±¼ = 1/m Ã— Î£áµ¢â‚Œâ‚áµ (hÎ¸(xáµ¢) - yáµ¢) Ã— xáµ¢â±¼
  
  â””â”€ POR QUÃ‰ X.T (TRANSPUESTA):
     - X: m Ã— n, (predictions - y): m Ã— 1
     - X.T: n Ã— m, permite multiplicaciÃ³n matricial
     - Resultado: n Ã— 1 (un gradiente por cada parÃ¡metro)
  â””â”€ QUE PASA SI LO CAMBIAS:
     âŒ gradient = (1/m) * X.dot(predictions - y): ERROR de dimensiones
     âŒ Sin (1/m): gradiente no normalizado - pasos muy grandes
  â””â”€ PROBLEMAS POTENCIALES:
     - Overflow si errores muy grandes
     - Underflow si caracterÃ­sticas muy pequeÃ±as

===============================================================================
4. IMPLEMENTACIÃ“N DEL ALGORITMO MINI-BATCH GRADIENT DESCENT
===============================================================================

LÃNEAS 92-134:

def mini_batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, batch_size=32, random_state=42):

EXPLICACIÃ“N DE PARÃMETROS:

â€¢ learning_rate=0.01
  â””â”€ QUE HACE: Controla el tamaÃ±o del paso en cada actualizaciÃ³n
  â””â”€ POR QUÃ‰ 0.01:
     - Valor tÃ­pico que funciona bien en muchos casos
     - No muy grande (inestabilidad) ni muy pequeÃ±o (lento)
  â””â”€ RANGO TÃPICO: [0.001, 0.1]
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… 0.1: Converge mÃ¡s rÃ¡pido, pero puede ser inestable
     âœ… 0.001: MÃ¡s estable, pero converge lentamente
     âŒ 1.0: Probable divergencia o oscilaciÃ³n
     âŒ 0: No aprende nada
  â””â”€ PROBLEMAS POTENCIALES:
     - Muy alto: divergencia, gradiente exploding
     - Muy bajo: convergencia lenta, puede quedarse en plateau
     - Negativo: se aleja del mÃ­nimo

â€¢ n_iterations=1000  
  â””â”€ QUE HACE: NÃºmero de Ã©pocas de entrenamiento
  â””â”€ QUÃ‰ ES UNA Ã‰POCA: Una pasada completa por todo el dataset
  â””â”€ POR QUÃ‰ 1000: Balance entre convergencia y tiempo de cÃ³mputo
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… 100: MÃ¡s rÃ¡pido, puede no converger completamente
     âœ… 5000: MÃ¡s tiempo, mejor convergencia
     âŒ 10: Muy pocas iteraciones - no aprende
     âŒ 0: No entrena
  â””â”€ PROBLEMAS POTENCIALES:
     - Muy pocas: underfitting
     - Demasiadas: overfitting, tiempo excesivo

â€¢ batch_size=32
  â””â”€ QUE HACE: NÃºmero de muestras procesadas en cada mini-batch
  â””â”€ POR QUÃ‰ 32: Potencia de 2, buen balance velocidad/estabilidad
  â””â”€ VALORES TÃPICOS: 8, 16, 32, 64, 128, 256
  â””â”€ QUE PASA SI LO CAMBIAS:
     âœ… 1: Se convierte en Stochastic GD
     âœ… len(X): Se convierte en Batch GD
     âœ… 64: MÃ¡s estable, pero requiere mÃ¡s memoria
     âŒ > len(X): Usa todos los datos (como Batch GD)
     âŒ 0: DivisiÃ³n por cero
  â””â”€ PROBLEMAS POTENCIALES:
     - Muy pequeÃ±o: convergencia ruidosa
     - Muy grande: convergencia lenta, alta memoria

EXPLICACIÃ“N DEL ALGORITMO PASO A PASO:

    np.random.seed(random_state)
    m, n = X.shape  
    theta = np.random.randn(n, 1)

â€¢ np.random.seed(random_state)
  â””â”€ QUE HACE: Asegura reproducibilidad dentro de la funciÃ³n
  â””â”€ POR QUÃ‰ AQUÃ: Controla inicializaciÃ³n de Î¸ y barajeo de datos
  â””â”€ PROBLEMAS POTENCIALES: Sin seed, resultados diferentes cada vez

â€¢ m, n = X.shape
  â””â”€ QUE HACE: Extrae dimensiones de la matriz de features
  â””â”€ m: nÃºmero de muestras
  â””â”€ n: nÃºmero de features (incluyendo bias)
  â””â”€ EJEMPLO: Para X_b shape=(1000, 2) â†’ m=1000, n=2

â€¢ theta = np.random.randn(n, 1)
  â””â”€ QUE HACE: Inicializa parÃ¡metros aleatoriamente
  â””â”€ POR QUÃ‰ ALEATORIO: Rompe simetrÃ­a, evita que todos los parÃ¡metros sean iguales
  â””â”€ np.random.randn: distribuciÃ³n normal estÃ¡ndar N(0,1)
  â””â”€ FORMA: (n, 1) - vector columna
  â””â”€ QUE PASA SI LO CAMBIAS:
     âŒ theta = np.zeros((n, 1)): Puede funcionar para regresiÃ³n lineal, pero problemÃ¡tico para redes neuronales
     âŒ theta = np.ones((n, 1)): InicializaciÃ³n sesgada
     âœ… theta = np.random.randn(n, 1) * 0.1: InicializaciÃ³n mÃ¡s pequeÃ±a
  â””â”€ PROBLEMAS POTENCIALES:
     - Valores muy grandes: gradiente exploding
     - Todos iguales: simetrÃ­a, aprende lentamente

    for epoch in range(n_iterations):
        # Barajar los datos al inicio de cada Ã©poca
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]

â€¢ for epoch in range(n_iterations):
  â””â”€ QUE HACE: Bucle principal de entrenamiento
  â””â”€ CADA ITERACIÃ“N: una Ã©poca completa

â€¢ indices = np.random.permutation(m)  
  â””â”€ QUE HACE: Genera permutaciÃ³n aleatoria de Ã­ndices [0, 1, ..., m-1]
  â””â”€ EJEMPLO: [0,1,2,3,4] â†’ [3,1,4,0,2]
  â””â”€ POR QUÃ‰ BARAJAR:
     - Evita sesgos por orden de los datos
     - Mejora convergencia
     - Simula muestreo aleatorio
  â””â”€ QUE PASA SIN BARAJAR:
     - Convergencia mÃ¡s lenta
     - Posible sesgo si datos estÃ¡n ordenados
     - Oscilaciones en funciÃ³n de costo

â€¢ X_shuffled = X[indices]
  Y_shuffled = y[indices]
  â””â”€ QUE HACE: Reordena datos segÃºn Ã­ndices aleatorios
  â””â”€ MANTIENE CORRESPONDENCIA: X_shuffled[i] corresponde a y_shuffled[i]
  â””â”€ POR QUÃ‰ CREAR COPIAS: No modifica datos originales

        # Procesar mini-batches
        for i in range(0, m, batch_size):
            # Obtener mini-batch
            end_idx = min(i + batch_size, m)
            X_batch = X_shuffled[i:end_idx]
            y_batch = y_shuffled[i:end_idx]

â€¢ for i in range(0, m, batch_size):
  â””â”€ QUE HACE: Itera sobre mini-batches con step=batch_size
  â””â”€ EJEMPLO: m=1000, batch_size=32
     - i = 0, 32, 64, ..., 992
     - Procesa 32 muestras en cada iteraciÃ³n

â€¢ end_idx = min(i + batch_size, m)
  â””â”€ QUE HACE: Asegura que no nos salgamos del rango de datos
  â””â”€ POR QUÃ‰ min(): El Ãºltimo batch puede ser mÃ¡s pequeÃ±o
  â””â”€ EJEMPLO: m=1000, batch_size=32, i=992
     - i + batch_size = 1024
     - min(1024, 1000) = 1000
     - Ãšltimo batch: Ã­ndices 992-1000 (8 muestras)

â€¢ X_batch = X_shuffled[i:end_idx]
  y_batch = y_shuffled[i:end_idx]
  â””â”€ QUE HACE: Extrae mini-batch actual
  â””â”€ SLICING: [i:end_idx] toma elementos desde i hasta end_idx-1

            # Calcular gradiente para el mini-batch
            gradient = compute_gradient(X_batch, y_batch, theta, len(X_batch))
            
            # Actualizar parÃ¡metros
            theta = theta - learning_rate * gradient

â€¢ gradient = compute_gradient(X_batch, y_batch, theta, len(X_batch))
  â””â”€ QUE HACE: Calcula gradiente solo para el mini-batch actual
  â””â”€ len(X_batch): tamaÃ±o real del batch (importante para Ãºltimo batch)
  â””â”€ DIFERENCIA CON BATCH GD: usa solo subset de datos

â€¢ theta = theta - learning_rate * gradient  
  â””â”€ QUE HACE: Actualiza parÃ¡metros en direcciÃ³n opuesta al gradiente
  â””â”€ REGLA DE ACTUALIZACIÃ“N: Î¸ := Î¸ - Î±âˆ‡J(Î¸)
  â””â”€ POR QUÃ‰ RESTAR: gradiente apunta hacia mÃ¡ximo, queremos mÃ­nimo
  â””â”€ MATEMÃTICA:
     Î¸â±¼ := Î¸â±¼ - Î± Ã— (1/batch_size) Ã— Î£áµ¢âˆˆbatch(hÎ¸(xáµ¢) - yáµ¢) Ã— xáµ¢â±¼

        # Calcular y almacenar el costo de toda la Ã©poca
        cost = compute_cost(X, y, theta)
        cost_history.append(cost)

â€¢ cost = compute_cost(X, y, theta)
  â””â”€ QUE HACE: Calcula costo usando TODOS los datos
  â””â”€ POR QUÃ‰ TODOS LOS DATOS Y NO SOLO EL BATCH:
     - Monitoreo consistente de progreso
     - ComparaciÃ³n justa entre Ã©pocas
     - DetecciÃ³n de overfitting
  â””â”€ COSTO COMPUTACIONAL: O(m) por Ã©poca
  â””â”€ ALTERNATIVA: Calcular solo cada N Ã©pocas para eficiencia

â€¢ cost_history.append(cost)
  â””â”€ QUE HACE: Almacena historial para anÃ¡lisis y visualizaciÃ³n
  â””â”€ PERMITE: GrÃ¡fico de convergencia, detecciÃ³n de problemas

===============================================================================
5. ALGORITMOS DE COMPARACIÃ“N (BATCH GD Y STOCHASTIC GD)  
===============================================================================

BATCH GRADIENT DESCENT:

def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, random_state=42):

DIFERENCIAS CLAVE CON MINI-BATCH:

â€¢ gradient = compute_gradient(X, y, theta)
  â””â”€ USA TODOS LOS DATOS en cada iteraciÃ³n
  â””â”€ NO HAY loop interno de batches
  â””â”€ VENTAJAS:
     - Convergencia suave
     - Gradiente exacto  
     - FÃ¡cil de implementar
  â””â”€ DESVENTAJAS:
     - Lento para datasets grandes
     - Alto uso de memoria
     - Puede quedarse en mÃ­nimos locales

STOCHASTIC GRADIENT DESCENT:

def stochastic_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, random_state=42):

DIFERENCIAS CLAVE:

    for epoch in range(n_iterations):
        indices = np.random.permutation(m)
        for i in indices:
            X_i = X[i:i+1]  # UNA SOLA MUESTRA
            y_i = y[i:i+1]
            gradient = compute_gradient(X_i, y_i, theta, 1)
            theta = theta - learning_rate * gradient

â€¢ X_i = X[i:i+1]
  â””â”€ QUE HACE: Toma UNA sola muestra (pero mantiene forma matricial)
  â””â”€ POR QUÃ‰ [i:i+1] Y NO [i]: Mantiene dimensiÃ³n (1,n) en lugar de (n,)
  â””â”€ BATCH_SIZE = 1

â€¢ for i in indices:
  â””â”€ ACTUALIZA PARÃMETROS para cada muestra individual
  â””â”€ m actualizaciones por Ã©poca (vs 1 en Batch GD)
  
COMPARACIÃ“N VISUAL:
- BATCH GD: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (una actualizaciÃ³n grande)
- MINI-BATCH: â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ (pocas actualizaciones medianas)  
- SGD: â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ â–ˆ (muchas actualizaciones pequeÃ±as)

===============================================================================
6. ANÃLISIS DE PROBLEMAS COMUNES Y SOLUCIONES
===============================================================================

PROBLEMA 1: DIVERGENCIA (Cost aumenta en lugar de disminuir)

CAUSAS:
â€¢ Learning rate muy alto
â€¢ Gradiente exploding
â€¢ Datos mal escalados
â€¢ InicializaciÃ³n pobre

SÃNTOMAS:
- cost_history muestra valores crecientes
- Valores de theta se vuelven muy grandes
- NaN o Inf en cÃ¡lculos

SOLUCIONES:
âœ… Reducir learning_rate (dividir por 10)
âœ… Normalizar/estandarizar datos
âœ… Usar gradient clipping
âœ… Mejor inicializaciÃ³n (Xavier, He)

CÃ“DIGO DE DIAGNÃ“STICO:
```python
if cost > prev_cost * 1.1:  # Cost aumentÃ³ 10%
    print("âš ï¸ POSIBLE DIVERGENCIA")
    learning_rate *= 0.5  # Reducir learning rate
```

PROBLEMA 2: CONVERGENCIA LENTA

CAUSAS:
â€¢ Learning rate muy bajo  
â€¢ Features con diferentes escalas
â€¢ InicializaciÃ³n cerca de saddle point
â€¢ Batch size inadecuado

SÃNTOMAS:
- cost_history disminuye muy lentamente
- Plateau en funciÃ³n de costo
- Muchas iteraciones sin mejora

SOLUCIONES:
âœ… Aumentar learning_rate gradualmente
âœ… Feature scaling/normalization
âœ… Adaptive learning rates (Adam, AdaGrad)
âœ… Momentum

PROBLEMA 3: OVERFITTING

CAUSAS:
â€¢ Demasiadas iteraciones
â€¢ Learning rate alto con muchas Ã©pocas
â€¢ Modelo muy complejo para datos

SÃNTOMAS:
- Training cost sigue bajando
- Validation cost comienza a subir
- Gran diferencia entre train/val

SOLUCIONES:  
âœ… Early stopping
âœ… RegularizaciÃ³n (L1, L2)
âœ… ValidaciÃ³n cruzada
âœ… Menos iteraciones

PROBLEMA 4: BATCH SIZE ISSUES

BATCH SIZE MUY PEQUEÃ‘O (â‰¤8):
â€¢ Convergencia muy ruidosa
â€¢ Ineficiente computacionalmente
â€¢ Dificulta paralelizaciÃ³n

BATCH SIZE MUY GRANDE (>512):
â€¢ Convergencia lenta
â€¢ Alto uso de memoria
â€¢ Puede quedarse en mÃ­nimos locales

BATCH SIZE Ã“PTIMO:
â€¢ Hardware: mÃºltiplo de 32 (GPUs)
â€¢ Dataset pequeÃ±o: 16-32
â€¢ Dataset grande: 64-256
â€¢ Regla general: âˆšm (raÃ­z del nÃºmero de muestras)

PROBLEMA 5: NUMERICAL INSTABILITY

CAUSAS:
â€¢ Valores muy grandes en X o y
â€¢ Underflow/overflow en cÃ¡lculos
â€¢ Mal condicionamiento de matriz

SÃNTOMAS:
- NaN o Inf en resultados
- Warnings de overflow
- Resultados inconsistentes

SOLUCIONES:
âœ… Feature scaling obligatorio
âœ… Usar np.float64 en lugar de float32
âœ… Gradient clipping
âœ… RegularizaciÃ³n

===============================================================================
7. RECOMENDACIONES Y MEJORES PRÃCTICAS
===============================================================================

SELECCIÃ“N DE HIPERPARÃMETROS:

LEARNING RATE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tipo Dataset    â”‚ Learning Rate â”‚ Notas           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PequeÃ±o (<1K)   â”‚ 0.01 - 0.1   â”‚ Puede ser alto  â”‚
â”‚ Mediano (1K-100K)â”‚ 0.001 - 0.01 â”‚ Balance tÃ­pico  â”‚
â”‚ Grande (>100K)  â”‚ 0.0001-0.001 â”‚ MÃ¡s conservador â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BATCH SIZE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Memoria/Hardware â”‚ Batch Size  â”‚ CaracterÃ­sticas  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU limitado     â”‚ 8 - 32      â”‚ Menos memoria    â”‚
â”‚ GPU estÃ¡ndar     â”‚ 32 - 128    â”‚ Balance Ã³ptimo   â”‚
â”‚ GPU potente      â”‚ 128 - 512   â”‚ MÃ¡xima eficienciaâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

NÃšMERO DE ITERACIONES:
â€¢ Usar early stopping en lugar de nÃºmero fijo
â€¢ Monitor validation loss
â€¢ Parar si no hay mejora en 50-100 Ã©pocas

PREPROCESSING OBLIGATORIO:
```python
# 1. Feature Scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. Train/Validation Split  
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)

# 3. Monitoreo
def train_with_validation(X_train, y_train, X_val, y_val):
    train_costs, val_costs = [], []
    for epoch in range(n_iterations):
        # Entrenamiento...
        train_cost = compute_cost(X_train, y_train, theta)
        val_cost = compute_cost(X_val, y_val, theta) 
        
        train_costs.append(train_cost)
        val_costs.append(val_cost)
        
        # Early stopping
        if val_cost > min(val_costs) * 1.01:  # 1% tolerance
            patience += 1
            if patience > 50:
                break
```

DEBUGGING CHECKLIST:

âœ… ANTES DE ENTRENAR:
- [ ] Datos normalizados/estandarizados
- [ ] Sin valores NaN o infinitos  
- [ ] Shapes correctas (X: mÃ—n, y: mÃ—1, theta: nÃ—1)
- [ ] Learning rate razonable
- [ ] Batch size vÃ¡lido

âœ… DURANTE EL ENTRENAMIENTO:
- [ ] Cost disminuye generalmente
- [ ] Theta no crece descontroladamente
- [ ] Sin warnings de overflow
- [ ] Tiempo de convergencia razonable

âœ… DESPUÃ‰S DEL ENTRENAMIENTO:
- [ ] MÃ©tricas finales razonables (RÂ², MSE)
- [ ] Predicciones visuales sensatas
- [ ] ParÃ¡metros interpretables

OPTIMIZACIONES AVANZADAS:

1. ADAPTIVE LEARNING RATE:
```python
def adaptive_learning_rate(cost_history, learning_rate, patience=10):
    if len(cost_history) > patience:
        recent_costs = cost_history[-patience:]
        if max(recent_costs) - min(recent_costs) < 1e-6:
            learning_rate *= 0.5  # Reduce if plateau
    return learning_rate
```

2. MOMENTUM:
```python
def mini_batch_with_momentum(X, y, learning_rate=0.01, momentum=0.9):
    velocity = np.zeros_like(theta)
    for epoch in range(n_iterations):
        # ... calcular gradient ...
        velocity = momentum * velocity + learning_rate * gradient
        theta = theta - velocity
```

3. LEARNING RATE SCHEDULING:
```python
def learning_rate_schedule(epoch, initial_lr=0.01):
    if epoch < 100:
        return initial_lr
    elif epoch < 200:
        return initial_lr * 0.1
    else:
        return initial_lr * 0.01
```

===============================================================================
8. CASOS DE USO Y CUÃNDO USAR CADA ALGORITMO
===============================================================================

BATCH GRADIENT DESCENT:
âœ… USAR CUANDO:
- Dataset pequeÃ±o (< 10,000 muestras)
- PrecisiÃ³n es crÃ­tica
- Recursos computacionales abundantes
- FunciÃ³n convexa garantizada

âŒ EVITAR CUANDO:
- Dataset muy grande (> 100,000 muestras)  
- Memoria limitada
- Necesidad de entrenamiento rÃ¡pido
- Datos de streaming

STOCHASTIC GRADIENT DESCENT:
âœ… USAR CUANDO:
- Dataset muy grande (> 1M muestras)
- Memoria muy limitada
- Entrenamiento online/streaming
- Necesidad de escapar mÃ­nimos locales

âŒ EVITAR CUANDO:
- Necesidad de convergencia suave
- AnÃ¡lisis de convergencia crÃ­tico
- Funciones muy ruidosas
- Hardware con buena paralelizaciÃ³n

MINI-BATCH GRADIENT DESCENT:
âœ… USAR CUANDO:
- Dataset mediano a grande (1K - 1M muestras)
- Hardware con GPU/paralelizaciÃ³n
- Balance velocidad-estabilidad importante
- MayorÃ­a de casos prÃ¡cticos

âŒ EVITAR CUANDO:
- Dataset extremadamente pequeÃ±o (< 100)
- Memoria extremadamente limitada
- Necesidad de mÃ¡xima precisiÃ³n

TABLA COMPARATIVA:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Algoritmo       â”‚ Velocidad  â”‚ Estabilidad      â”‚ Uso de Memoria  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Batch GD        â”‚ Lento      â”‚ Muy Alta         â”‚ Alto            â”‚
â”‚ Mini-Batch GD   â”‚ Medio      â”‚ Alta             â”‚ Medio           â”‚  
â”‚ Stochastic GD   â”‚ RÃ¡pido     â”‚ Baja             â”‚ Bajo            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

===============================================================================
9. EXTENSIONES Y MEJORAS FUTURAS
===============================================================================

ALGORITMOS AVANZADOS:
â€¢ Adam (Adaptive Moment Estimation)
â€¢ RMSprop (Root Mean Square Propagation)  
â€¢ AdaGrad (Adaptive Gradient)
â€¢ BFGS (Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno)

REGULARIZACIÃ“N:
â€¢ L1 Regularization (Lasso)
â€¢ L2 Regularization (Ridge)  
â€¢ Elastic Net (L1 + L2)
â€¢ Dropout (para redes neuronales)

TÃ‰CNICAS DE CONVERGENCIA:
â€¢ Early Stopping
â€¢ Learning Rate Decay
â€¢ Cyclical Learning Rates
â€¢ Warm Restarts

PARALELIZACIÃ“N:
â€¢ Data Parallelism
â€¢ Model Parallelism
â€¢ Asynchronous SGD
â€¢ Federated Learning

===============================================================================
RESUMEN EJECUTIVO
===============================================================================

El algoritmo Mini-Batch Gradient Descent implementado:

âœ… CARACTERÃSTICAS CLAVE:
- Procesa datos en lotes de tamaÃ±o configurable
- Barajea datos en cada Ã©poca para mejor convergencia
- Incluye mediciÃ³n de tiempo y monitoreo de costo
- ImplementaciÃ³n vectorizada para eficiencia

âœ… PARÃMETROS CRÃTICOS:
- learning_rate: 0.01 (ajustar segÃºn dataset)
- batch_size: 32 (potencia de 2 para eficiencia)
- n_iterations: monitorear convergencia

âœ… VENTAJAS IMPLEMENTADAS:
- Balance Ã³ptimo velocidad/estabilidad  
- Aprovecha vectorizaciÃ³n de NumPy
- CÃ³digo modular y reutilizable
- ComparaciÃ³n directa con otras variantes

âš ï¸ CONSIDERACIONES IMPORTANTES:
- Requiere feature scaling para Ã³ptimo rendimiento
- Batch size debe ajustarse segÃºn memoria disponible
- Learning rate crÃ­tico para convergencia
- Monitoreo constante necesario para detectar problemas

ğŸ¯ RESULTADO ESPERADO:
Con los datos sintÃ©ticos (y = 4 + 3x + ruido), el algoritmo debe converger a:
- Î¸â‚€ â‰ˆ 4.0 (intercepto)  
- Î¸â‚ â‰ˆ 3.0 (pendiente)
- MSE < 1.0 (dependiendo del ruido)

===============================================================================
FIN DEL DOCUMENTO - EXPLICACIÃ“N COMPLETA MINI-BATCH GRADIENT DESCENT
===============================================================================